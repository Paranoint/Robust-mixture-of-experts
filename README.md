# Robust-mixture-of-experts
Магистерская диссертация на тему "Робастные методы оценивания параметров регрессионной смеси экспертов"

## Генерация синтетических регрессионных данных

`moe_data_generator.py` теперь реализует простой генератор синтетических данных для линейной регрессии (1D или 2D вход), который сохраняет результат в CSV для дальнейшей обработки.

Пример использования:

```bash
python moe_data_generator.py data.csv --samples 500 --input-dim 2 --noise-std 0.2 --seed 13
```

Команда создаст `data.csv` с колонками `x1, x2, y`. Истинные параметры (жестко заданные веса и смещение) выводятся в консоль. При необходимости скорректируйте словарь `DEFAULT_PARAMS` внутри скрипта, чтобы использовать другие коэффициенты.

Для моделирования выбросов доступны параметры `--outlier-frac` (доля испорченных точек) и `--outlier-scale` (мощность шума выбросов). Пример:

```bash
python moe_data_generator.py data.csv --outlier-frac 0.15 --outlier-scale 8.0
```

Такая команда заменит примерно 15% наблюдений сильным шумом, что удобно для тестирования устойчивости алгоритмов.

## Обучение MoE на CSV (C++)

`moe_trainer.cpp` реализует обучение двух вариантов смеси экспертов по CSV-файлу (последний столбец — отклик, остальные признаки):

- стандартная MoE с EM и гауссовским шумом;
- робастная MoE с γ-дивергенцией (EEE-подобное взвешивание).

Сборка и запуск:

```bash
g++ -std=c++17 -O2 -I /usr/include/eigen3 dataset.cpp moe_training.cpp moe_trainer.cpp -o moe_trainer
./moe_trainer data.csv --experts 3 --iters 60 --gamma 0.4
```

Программа выводит RMSE, показатель устойчивости (отношение MSE для верхних 10% резидуалов к общему MSE) и итоговые веса/дисперсии экспертов для обоих вариантов, что позволяет сравнить качество и чувствительность к выбросам.
